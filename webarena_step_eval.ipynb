{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write code that loads in all \"summary.csv\" files in all subdirectories of a given directory and concatenates them into a single dataframe\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "def load_summary_files(directory):\n",
    "    dfs = []\n",
    "    for root, dirs, files in os.walk(directory):\n",
    "        for file in files:\n",
    "            if file == \"summary.csv\":\n",
    "                df = pd.read_csv(os.path.join(root, file))\n",
    "                dfs.append(df)\n",
    "    return pd.concat(dfs)\n",
    "\n",
    "results_full = load_summary_files(\"webagents-step-main/data/webarena/eval/step_full/gpt-4-turbo-2024-04-09/run1\")\n",
    "results_reddit = load_summary_files(\"webagents-step-main/data/webarena/eval/step_reddit_unmodified/gpt-4-turbo-2024-04-09/run1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{598}\n"
     ]
    }
   ],
   "source": [
    "# check which tasks are missing\n",
    "reddit_ids = [27, 28, 29, 30, 31, 66, 67, 68, 69, 399, 400, 401, 402, 403, 404, 405, 406, 407, 408, 409, 410, 552, 553, 554, 555, 562, 563, 564, 565, 566, 580, 581, 582, 583, 584, 595, 596, 597, 598, 599, 600, 601, 602, 603, 604, 605, 606, 607, 608, 609, 610, 611, 612, 613, 614, 615, 616, 617, 618, 619, 620, 621, 622, 623, 624, 625, 626, 627, 628, 629, 630, 631, 632, 633, 634, 635, 636, 637, 638, 639, 640, 641, 642, 643, 644, 645, 646, 647, 648, 649, 650, 651, 652, 671, 672, 673, 674, 675, 681, 682, 683, 684, 685, 686, 687, 688, 714, 715, 716, 717, 718, 719, 720, 721, 722, 723, 724, 725, 726, 727, 728, 729, 730, 731, 732, 733, 734, 735, 791]\n",
    "missing_tasks = set(reddit_ids) - set(results_reddit[\"task_id\"])\n",
    "print(missing_tasks)\n",
    "\n",
    "# one task could not be completed on reddit by the agent that raised error. We are counting this is failure whe computing the success rate below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "812\n",
      "128\n"
     ]
    }
   ],
   "source": [
    "# check that all tasks have been completed\n",
    "print(results_full.task_id.nunique())\n",
    "print(results_reddit.task_id.nunique()) # one task could not be completed and agent raised error\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Success Rate STeP full benchmark:  0.24753694581280788\n",
      "Success Rate STeP Reddit benchmark:  0.21705426356589147\n"
     ]
    }
   ],
   "source": [
    "\n",
    "(results_full.success.sum())/812\n",
    "(results_reddit.success.sum())/129\n",
    "\n",
    "print(\"Success Rate STeP full benchmark: \", results_full.success.sum()/812)\n",
    "print(\"Success Rate STeP Reddit benchmark: \", results_reddit.success.sum()/129)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## the below code block generates a dict with all task_ids per website that have that website as start url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for all .json files in a directoy, load the file and extract the \"task_id\". append the task_ids to a dict that is populated with the keys being the first item in the list \"sites\" that is part of each json file\n",
    "import json\n",
    "import os\n",
    "\n",
    "task_ids = {}\n",
    "for root, dirs, files in os.walk(\"/scratch/gpfs/bs6865/agent-eval/webagents-step-main/tasks/webarena\"):\n",
    "    for file in files:\n",
    "        if file.endswith(\".json\"):\n",
    "            with open(os.path.join(root, file)) as f:\n",
    "                data = json.load(f)\n",
    "                site = data[\"sites\"][0]\n",
    "                if site == \"wikipedia\":\n",
    "                    site = data[\"sites\"][1]\n",
    "                task_id = data[\"task_id\"]\n",
    "                if site not in task_ids:\n",
    "                    task_ids[site] = []\n",
    "                task_ids[site].append(task_id)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "novelqa",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
